\documentclass{article}
\usepackage{fullpage}
\usepackage{bold-extra}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{url}
\usepackage{here}
\usetikzlibrary{automata,arrows,calc,positioning}
\lstset{ frame=single, breaklines=true}
\title{Final Project Report:\\Titanic: Machine Learning from Disaster}
\author{Fengfei Zheng, Ming-hung Shih, Shotaro Matsui}
\date{\today}
\begin{document}
\maketitle

% \fontsize{11pt} {0cm}\selectfont
\section{Introduction}
"Titanic: Machine Learning from Disaster" is an active Kaggle competition on
\url{http://www.kaggle.com/titanic}.

The purpose of the problem is to predict what sorts of people were likely to survive, using their personal information.
So this problem is a binary classification problem.
The output is $\{0,1\}$, whether survive or not, and the input is features listed below:
\begin{description}
    \item[pclass] Ticket class
    \item[name] passenger's name with title
    \item[sex] sex
    \item[age] Age in years
    \item[sibsp] \# of siblings and spouses aboard the Titanic
    \item[parch] \# of parents and children aboard the Titanic
    \item[ticket] Ticket number
    \item[fare] Passenger fare
    \item[cabin] Cabin number
    \item[embarked] Port of Embarkation
\end{description}
There are 891 training examples (with survived/not-survived label) and 418 test data (without that label), so 1309 data for total.
The survival rate on training set was about $38\%$.




\section{Existing Approach}
Since this is a classical binary classification problem, this problem seems to be a practice problem or playground to study machine learning basics.
So there are nice tutorials on the overview section on the Kaggle web page, and also coultless Kernels which we cannot check all of them.
% Some of them are 
So we decided that each member choose one or more Kernels and go through, combine methods or try new methods so that we could improve the accuracy.

Referenced Kernels:\\
\url{https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling}


\section{Our Approach}
\subsection{Feature analysis/engineering}

\subsubsection*{outliers}
At first, extracted outliers.
For each numerical value features (age, sibsp, parch and fare), compute interquartile range (IQR), and ommit examples which has a value outside of this range.
There are 10 of outliers.
So the total number of data (to analyze) is 1299. 
\textcolor{red}{Also, there are some missed data in the data-set and we should predict them with corresponding data.}

\subsubsection*{pclass(categorical)}
Categorical value that ranges 1st to 3rd class.
The higher the class, the more likely to survive.

\subsubsection*{Name(categorical)}
Passengers' names contains their titles, which may affect survivals.
Besides titles to differenciate genders (i.e. Mr., Mrs., Mlle, etc.), there are some titles like Master., Dr., or else.
People who has those titles were more likely to survive.
So extracting those titles and form a new categorical value\\
 $(Male:Mr.)$\\
 $(Female:Miss., Ms., Mrs., Mme., Mlle)$\\
 $(Master:Master.)$\\
 $(Other\mathchar`-titles:Lady., Countess., Dr., Capt., Rev., Col., Don., Major., Sir., Jonkheer., Dona.)$\\
The reason why Master is differenciated from Others-titles is because it was much more frequent than others (Master:62, Other-titles:29 for total), and also Masters. was more likely to survive than Other-titles passengers.

\subsubsection*{sex(categorical)}
Female is more likely to survive.
Actually, $74\%$ of female is survived while male survival rate is only $19\%$.

\subsubsection*{age(numerical)}
By comparing survival and non-survival group, very young (around 0-5) children are more likely to survive, and very old (around 60-80) people are less likely to survive.

And there are 256 missing values, so estimated them from other numerical features, sibsp, parch and pclass.
Filled missing ages using median of rows which have same(sibsp, parch and pclass).
If there is not such row, fill with median age.

\subsubsection*{sibsp, parch(numerical) $\rightarrow$ fsize(categorical)}
Both of the value has similar effects on survival.
Passengers who has middle size (1-2) of sibsp/parch are more likely to survive.
So combined these two features and introduced a new feature "fsize (family size)", which is $(fsize) = (sibsp)+(parch) +1$, where $+1$ means the passenger himself.
And then turned the numerical value to categorical value,
$Single=1, SmallF=2, MedF=(3,4), LargeF=(5,\cdots)$.

\subsubsection*{ticket(categorical)}
The ticket number would contains information about pclass or cabin.
As shown in pclass section, higher class passengers are more likely to survive.
So, to extract that information, using only prefix of ticket number and dropping its numbers. 
\textcolor{red}{Some ticket number are duplicate and it shows that this passenger took Titanic with others. We tried mark the passenger whose ticket number are the same as a group and no duplicate number as the other group to predict their survival rate.}

\subsubsection*{fare(numerical)}
There is one missing value, so decided to fill it with midian value.
The distribution of fare is ranged around 0-500, however most of passengers are around between 0-25, so decided to use $log(fare)$ value to differenciate those values more precisely.

\subsubsection*{cabin(categorical)}
$1007/1299$ of passengers do not have this value (missing value), which would mean that they did not have cabins.
So introducing new category "X" to express that.
Also omitting cabin number from the prefix (e.g. C123 $\rightarrow$ C) to reduce unnecessary dimentionality.

\subsubsection*{embarked(categorical)}
The two missing values are filled out with the most frequent value, S (C=Cherbourg, Q=Queenstown, S=Southampton).
The place of embarkment is related to pclass.
Indeed, the survival rate was $C>Q>S$, and over half of S and Q embarked passengers are 3rd class, while half of C passengers are 1st class.

\section{Experiments}
\subsection{Linear SVM}
\textcolor{red}{For linear kernel, with different C it took about 1 minute to complete 891 training data. The error rate is about 74\% to 77\%.}
\subsection{Quadratic SVM}
For quadratic kernel, with different C it took about 5 minutes and the error rate is about 74\% to 78\%. 
\subsection{Gaussian SVM}
For gaussian kernel, with different C it took about ?? minutes and the error rate is about ??\% to ??\%. (too long)

\section{Results and Discussion}
\subsection{Cross-Validation}
\textcolor{red}{To evaluate the result before submitting to Kaggle, we tried cross validation and found the error rate is lower to 8\% because of overfitting. With the submission limit of Kaggle, we should do more data analysis to make the cross validation more meaningful.}

\subsection{Analysis conclusion}
The most effective features were sex, fare and name (title).
Female, very young children, people who has Social status (title) and having higher ticket class are likely to survive.
Male, old people, and lower class ticket holders are less likely to survive.
It meets intuition that female, children and socially important people are priorly evacuated.

\subsection{Voting classificator}
Using scikit-learn tools, trained five different algorithms, SVM (SVC) with gaussian kernel ( 'rbf' ), Random Forest, Extremely Random Tree, Ada Boosting and Gradient Boosting with Grid Search function to find the best tuned ones.
The result was that all of these are around $82\mathchar`-83\%$ accuracy on cross-validation, so constructed a voting classifier using VotingClassifier() function (with 'soft' option) and got $92\%$ cross validation accuracy.

The Kaggle score was $78\%$ accuracy, which was $4013/9550$.
Some other approaches like not omitting outliers, or combining Masters. and Other-titles in the name feature, did not help improving both cross-validation score or test score (Kaggle submission).

Also we have to state the top rankers on Kaggle are $100\%$ accuracy, which is doubtful.
Probably because the test set has only 418 examples, they could got that accuracy?

\section{Conclusion}
We tried to solve a basic binary classification problem, and found out that the most part of efforts on machine learning is feature engineering.
How to transform and treat training examples is difficult problem, especially if training takes very long time, because we would not be able to check results so frequently.
So we would have to try figuring out what relationship of cause and effect is behind features (and label) before actually trying learning methods.


\end{document}